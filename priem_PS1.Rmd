---
title: "Homework 1: Bias, Variance, & Resampling"
subtitle: |
  | MACS 30100: Perspectives on Computational Modeling
  | University of Chicago
author: "Betsy Priem"
output: pdf_document
---

## Overview

For each of the following prompts, produce responses _with_ code in-line. While you are encouraged to stage and draft your problem set solutions using any files, code, and data you'd like within the private repo for the assignment, *only the final, rendered PDF with responses and code in-line will be graded.*

## Bias & Variance

1. (10 points) Consider the following eight plots based on model fits, assuming the data generating process, $$y = x^3 - 2x^2 + 1.5x + \epsilon.$$ Specifically, the figure shows two different fits of a model (one for each row) on each of the four samples (one for each column). So, e.g., column 1 shows two versions of a model fit to the same sample of 100 observations, which were drawn at random from the data generating process in the equation above. *Describe the difference between the two models in terms of complexity, bias, and variance. Responses should be at least a few sentences.*

![Two models; Four Random Samples](images/regfits.png)
**The top row shows a linear model (less complexity) that has higher bias and lower variance. This means that each observation will likely deviate more from the model (representing the *higher bias* and mistakes in the linear model's ability to make accurate predictions), while the linear model fit will likely make more consistent predictions across samples because the error the model generates will not be as reactive to different distributions of data (i.e., the *lower variance*). The bottom row illustrates a more complex, nonlinear model fit of the data. The trade-off with using this more complex modeling is that while bias (mistakes in predictions) is much lower, variance is far higher compared to the linear, less complex models - making it more error prone against different random samples.**
 
2. (10 points) Building on the previous question and considering the following figure from ch. 2 of the ISL book, think about training and test error moving from a less flexible model towards more a more flexible model. Specifically, the figure is becoming more "flexible" as the number of neighbors, $k$, decreases, thereby picking up more local behavior. We haven't yet covered kNN or other supervised classifiers, but the logic of the figure should be apparent, where the level of flexibility of a model will directly influence training and testing error. *Explain why these two curves have the shapes they do. Responses should be at least a few sentences.*

![Figure 2.17 from ISL](images/flex.png)
**The training curve shows a decline as the flexibility increases, which makes sense because the increased flexibility means the model will more closely fit the data (decreasing the error rate). The test curve is U-shaped because with high restrictions (i.e., the least flexible model) the model will be highly inaccurate and have a higher error rate. At the optimal level of flexibility (in this case at around k=10), the model will make the fewest mistakes and generate a minimal error rate, representing the dip in the U-shape. The right-hand part of the U-shape indicates that with increasing flexibility this method generates a model that will overfit the data and become much more prone to error (signaled by the increase in the error rate).**

3. (10 points) When the sample size, $n$, is very large, and the number of predictors, $p$, is small, would we expect the performance of a flexible model to be better or worse than an inflexible method? Justify your answer.

**With a large sample size and small number of predictors you would expect the flexible model to do better than an inflexible model. This is because the flexible model would fit the data better and have a lower MSE. An inflexible model (like a linear regression) would likely be highly biased because fitting a straight line through a large sample that likely is more heterogeneous than a sample with a smaller *n* will result in creating a fit that has observations that are far from that line.**

4. (10 points) When the number of predictors, $p$ is very large, and the sample size, $n$, is small, would we expect the performance of a flexible model to be better or worse than an inflexible method? Justify your answer.

**In this case, the flexible model would perform worse compared to the inflexible method. The flexible method would likely overfit the data because of the greater number of predictors and small sample size. In trying to use all the predictors to explain the relationship between so few observations, the complex model would likely model associations that have no real bearing on the "real-world associations" (i.e., it would *overfit* the data). An inflexible model would stay truer to the expected "real-world associations" because it would be less sensitive to the higher number of predictors than the flexible method.** 

5. (10 points) When the relationship between the predictors, $\mathbf{X}$, and response, $y$, is highly non-linear, would we expect the performance of a flexible model to be better or worse than an inflexible method? Justify your answer.

**With a non-linear relationship between predictors and the response, a flexible model would be better. While the flexible method would complicate interpretability, it would allow for better prediction because of the non-linear relationship between predictors and the response. However, there is a danger of overfitting the data with the flexible model, in which case the inflexible model would offer both easier an easier to interpret estimate of *f* and more accurate predictions.**

6. (10 points) Why can minimizing the training mean squared error (MSE) lead to overfitting? *Responses should be at least a few sentences.*

**You can minimize the training MSE by increasing the model flexibility, which means you can closely fit the model to the observations in the training sample. But when applying this estimated function with the minimized training MSE to the test set, the function might be tuned too specifically to the unique characteristics of the training set (i.e., "overfit") and it will generate a much larger *test* MSE than the smaller *training* MSE. Basically, the model only does well with the training set and because it too closely models those specific observations (*overfitting*) it can no longer do well in samples that are different from the training set.** 

7. (10 points) Recall bootstrapping involves a process of drawing random samples of size $n$ through sampling *with* replacement. Create and plot two bootstrapped samples manually (i.e., *not using a function like `boot()`*). For reference, also plot the original data set to compare distributions. *Note:* When loading the data, you may simply drop the `NA`s, rather than impute, for ease. 

8. (5 points) Discuss the distributions from the previous question. Do they look mostly similar as is expected by sampling with replacement? Why or why not, do you think? *Respond with a few sentences.*

9. (15 points) The median for feelings toward Obama (`ftobama`) is 39.5. Using a package or any function/method you'd like, bootstrap the standard error of our statistic of interest (which is the median in this case) based on 1000 draws from the data. You might consider writing a simple helper function to speed along the bootstrapping process, but this is up to you of course. Then, construct the 95% confidence interval around your bootstrapped estimate. Report your results and offer a few points of discussion. *Respond with a few sentences.*

10. (10 points) How are bootstrapping and cross-validation approaches to resampling different? How are they similar? Why does any of this matter from both social science and computational modeling perspectives?
